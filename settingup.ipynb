{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "db = pd.read_csv('responses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding whether each poll has an optimum solution\n",
    "model= db[~db['score'].isin([1,2,3,4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def model_answer(poll,multiple=False):\n",
    "    answers = db[db['pollid']==poll]\n",
    "    answers = shuffle(answers)\n",
    "    answers.sort_values(by='score', ascending=False, inplace=True)\n",
    "    return answers if multiple else answers.iloc[0]['response']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We were arguing in my breakout about how we can know that moral objectivism even exists as there is no empirical evidence (similar to how we cannot empirically prove if there is a deity or the like). However, plato's objectivism comes from logical consistency and proof rather than observational proo\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_answer(12522,True).iloc[0]['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceApi\n",
    "\n",
    "inference = InferenceApi(\"google/flan-t5-xl\",token='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')\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "def infer(prompt,\n",
    "          max_length = 250,\n",
    "          top_k = 0,\n",
    "          num_beams = 0,\n",
    "          no_repeat_ngram_size = 2,\n",
    "          top_p = 0.7,\n",
    "          seed= random.randint(0,1000000),\n",
    "          temperature=0.01,\n",
    "          greedy_decoding = False,\n",
    "          return_full_text = False):\n",
    "    \n",
    "\n",
    "    top_k = None if top_k == 0 else top_k\n",
    "    do_sample = False if num_beams > 0 else not greedy_decoding\n",
    "    num_beams = None if (greedy_decoding or num_beams == 0) else num_beams\n",
    "    no_repeat_ngram_size = None if num_beams is None else no_repeat_ngram_size\n",
    "    top_p = None if num_beams else top_p\n",
    "    early_stopping = None if num_beams is None else num_beams > 0\n",
    "\n",
    "    params = {\n",
    "        \"top_k\": top_k,\n",
    "        \"top_p\": top_p,\n",
    "        \"temperature\": temperature,\n",
    "        \"do_sample\": do_sample,\n",
    "        \"early_stopping\":early_stopping,\n",
    "        \"no_repeat_ngram_size\":no_repeat_ngram_size,\n",
    "        \"num_beams\":num_beams\n",
    "    }\n",
    "    \n",
    "    s = time.time()\n",
    "    response = inference(prompt, params=params)\n",
    "    #print(response)\n",
    "    proc_time = time.time()-s\n",
    "    #print(f\"Processing time was {proc_time} seconds\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = db.sort_values(by='pollid')\n",
    "poll_ids = db['pollid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, Text, Integer, ForeignKey\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import relationship, sessionmaker \n",
    "\n",
    "engine = create_engine('sqlite:///database.db')\n",
    "engine.connect() \n",
    "\n",
    "Base = declarative_base() \n",
    "\n",
    "class Question(Base):\n",
    "\t__tablename__ = 'users'\n",
    "\tpollid = Column(Integer, primary_key = True)\n",
    "\tname = Column(Text)\n",
    "\tdef __repr__(self):\n",
    "\t\treturn \"<Question(id={0}, question={1})>\".format(self.pollid, self.name)\n",
    "\n",
    "Base.metadata.create_all(bind=engine) \n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting the database\n",
    "session.query(Question).delete()\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt_from_answers(pollid):\n",
    "    answers = db[db['pollid']==pollid]\n",
    "    answers = shuffle(answers)\n",
    "    answers.sort_values(by='score', ascending=False, inplace=True)\n",
    "    prompt = '''\n",
    "    Prompt 1 : {0}\n",
    "\n",
    "    Prompt 2 : {1}\n",
    "    \n",
    "    Prompt 3 : {2}\n",
    "\n",
    "    Question: What questions are the above three prompts answering?\n",
    "    \n",
    "    Answer: '''.format(answers.iloc[0]['response'],answers.iloc[1]['response'], answers.iloc[2]['response'])\n",
    "    return prompt\n",
    "\n",
    "j = 0\n",
    "for i in poll_ids:\n",
    "    j += 1\n",
    "    if j == 10:\n",
    "        break\n",
    "    if session.query(Question).filter_by(pollid=i).first() is None:\n",
    "        prompt = make_prompt_from_answers(i)\n",
    "        resp = infer(prompt)\n",
    "        question = Question(pollid=int(i), name=resp[0]['generated_text'])\n",
    "        session.add(question)\n",
    "        session.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same thing but with Bloom #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceApi\n",
    "\n",
    "inference = InferenceApi(\"bigscience/bloom\",token='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')\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "def infer(prompt,\n",
    "          max_length = 250,\n",
    "          top_k = 50,\n",
    "          num_beams = 0,\n",
    "          no_repeat_ngram_size = 0,\n",
    "          top_p = 1,\n",
    "          seed= random.randint(0,1000000),\n",
    "          temperature=0.2,\n",
    "          greedy_decoding = False,\n",
    "          return_full_text = False):\n",
    "    \n",
    "\n",
    "    top_k = None if top_k == 0 else top_k\n",
    "    do_sample = False if num_beams > 0 else not greedy_decoding\n",
    "    num_beams = None if (greedy_decoding or num_beams == 0) else num_beams\n",
    "    no_repeat_ngram_size = None if num_beams is None else no_repeat_ngram_size\n",
    "    top_p = None if num_beams else top_p\n",
    "    early_stopping = None if num_beams is None else num_beams > 0\n",
    "\n",
    "    params = {\n",
    "        \"max_new_tokens\": max_length,\n",
    "        \"top_k\": top_k,\n",
    "        \"top_p\": top_p,\n",
    "        \"temperature\": temperature,\n",
    "        \"do_sample\": do_sample,\n",
    "        \"seed\": seed,\n",
    "        \"early_stopping\":early_stopping,\n",
    "        \"no_repeat_ngram_size\":no_repeat_ngram_size,\n",
    "        \"num_beams\":num_beams,\n",
    "        \"return_full_text\":return_full_text,\n",
    "        \"n_gram_size\": 1\n",
    "    }\n",
    "    \n",
    "    s = time.time()\n",
    "    response = inference(prompt, params=params)\n",
    "    #print(response)\n",
    "    proc_time = time.time()-s\n",
    "    #print(f\"Processing time was {proc_time} seconds\")\n",
    "    return response\n",
    "\n",
    "\n",
    "# j = 0\n",
    "# for i in poll_ids:\n",
    "#     j += 1\n",
    "#     if j == 10:\n",
    "#         break\n",
    "#     if session.query(Question).filter_by(pollid=i).first() is None:\n",
    "#         prompt = make_prompt_from_answers(i)\n",
    "#         resp = infer(prompt)\n",
    "#         question = Question(pollid=int(i), name=resp[0]['generated_text'][len(prompt):])\n",
    "#         session.add(question)\n",
    "#         session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = infer('''\n",
    "Prompt 1 : What is the best way to get rid of a cold?\n",
    "''')\n",
    "print(res[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "526a7a986be34c6aa5d2765e86ea11ad1e541f44b43fc99ae4544a937b9b911e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
