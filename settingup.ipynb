{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "db = pd.read_csv('responses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding whether each poll has an optimum solution\n",
    "model= db[~db['score'].isin([1,2,3,4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def model_answer(poll,multiple=False):\n",
    "    answers = db[db['pollid']==poll]\n",
    "    answers = shuffle(answers)\n",
    "    answers.sort_values(by='score', ascending=False, inplace=True)\n",
    "    return answers if multiple else answers.iloc[0]['response']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We were arguing in my breakout about how we can know that moral objectivism even exists as there is no empirical evidence (similar to how we cannot empirically prove if there is a deity or the like). However, plato's objectivism comes from logical consistency and proof rather than observational proo\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_answer(12522,True).iloc[0]['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceApi\n",
    "\n",
    "inference = InferenceApi(\"google/flan-t5-xl\",token='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')\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "def infer(prompt,\n",
    "          max_length = 250,\n",
    "          top_k = 0,\n",
    "          num_beams = 0,\n",
    "          no_repeat_ngram_size = 2,\n",
    "          top_p = 0.7,\n",
    "          seed= random.randint(0,1000000),\n",
    "          temperature=0.01,\n",
    "          greedy_decoding = False,\n",
    "          return_full_text = False):\n",
    "    \n",
    "\n",
    "    top_k = None if top_k == 0 else top_k\n",
    "    do_sample = False if num_beams > 0 else not greedy_decoding\n",
    "    num_beams = None if (greedy_decoding or num_beams == 0) else num_beams\n",
    "    no_repeat_ngram_size = None if num_beams is None else no_repeat_ngram_size\n",
    "    top_p = None if num_beams else top_p\n",
    "    early_stopping = None if num_beams is None else num_beams > 0\n",
    "\n",
    "    params = {\n",
    "        \"top_k\": top_k,\n",
    "        \"top_p\": top_p,\n",
    "        \"temperature\": temperature,\n",
    "        \"do_sample\": do_sample,\n",
    "        \"early_stopping\":early_stopping,\n",
    "        \"no_repeat_ngram_size\":no_repeat_ngram_size,\n",
    "        \"num_beams\":num_beams\n",
    "    }\n",
    "    \n",
    "    s = time.time()\n",
    "    response = inference(prompt, params=params)\n",
    "    #print(response)\n",
    "    proc_time = time.time()-s\n",
    "    #print(f\"Processing time was {proc_time} seconds\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = db.sort_values(by='pollid')\n",
    "poll_ids = db['pollid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, Text, Integer, ForeignKey\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import relationship, sessionmaker \n",
    "\n",
    "engine = create_engine('sqlite:///database.db')\n",
    "engine.connect() \n",
    "\n",
    "Base = declarative_base() \n",
    "\n",
    "class Question(Base):\n",
    "\t__tablename__ = 'users'\n",
    "\tpollid = Column(Integer, primary_key = True)\n",
    "\tname = Column(Text)\n",
    "\tdef __repr__(self):\n",
    "\t\treturn \"<Question(id={0}, question={1})>\".format(self.pollid, self.name)\n",
    "\n",
    "Base.metadata.create_all(bind=engine) \n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting the database\n",
    "session.query(Question).delete()\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt_from_answers(pollid):\n",
    "    answers = db[db['pollid']==pollid]\n",
    "    answers = shuffle(answers)\n",
    "    answers.sort_values(by='score', ascending=False, inplace=True)\n",
    "    prompt = '''\n",
    "    Prompt 1 : {0}\n",
    "\n",
    "    Prompt 2 : {1}\n",
    "    \n",
    "    Prompt 3 : {2}\n",
    "\n",
    "    Question: What questions are the above three prompts answering?\n",
    "    \n",
    "    Answer: '''.format(answers.iloc[0]['response'],answers.iloc[1]['response'], answers.iloc[2]['response'])\n",
    "    return prompt\n",
    "\n",
    "j = 0\n",
    "for i in poll_ids:\n",
    "    j += 1\n",
    "    if j == 10:\n",
    "        break\n",
    "    if session.query(Question).filter_by(pollid=i).first() is None:\n",
    "        prompt = make_prompt_from_answers(i)\n",
    "        resp = infer(prompt)\n",
    "        question = Question(pollid=int(i), name=resp[0]['generated_text'])\n",
    "        session.add(question)\n",
    "        session.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same thing but with Bloom #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceApi\n",
    "\n",
    "inference = InferenceApi(\"bigscience/bloom\",token='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')\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "def infer(prompt,\n",
    "          max_length = 250,\n",
    "          top_k = 50,\n",
    "          num_beams = 0,\n",
    "          no_repeat_ngram_size = 0,\n",
    "          top_p = 1,\n",
    "          seed= random.randint(0,1000000),\n",
    "          temperature=0.2,\n",
    "          greedy_decoding = False,\n",
    "          return_full_text = False):\n",
    "    \n",
    "\n",
    "    top_k = None if top_k == 0 else top_k\n",
    "    do_sample = False if num_beams > 0 else not greedy_decoding\n",
    "    num_beams = None if (greedy_decoding or num_beams == 0) else num_beams\n",
    "    no_repeat_ngram_size = None if num_beams is None else no_repeat_ngram_size\n",
    "    top_p = None if num_beams else top_p\n",
    "    early_stopping = None if num_beams is None else num_beams > 0\n",
    "\n",
    "    params = {\n",
    "        \"max_new_tokens\": max_length,\n",
    "        \"top_k\": top_k,\n",
    "        \"top_p\": top_p,\n",
    "        \"temperature\": temperature,\n",
    "        \"do_sample\": do_sample,\n",
    "        \"seed\": seed,\n",
    "        \"early_stopping\":early_stopping,\n",
    "        \"no_repeat_ngram_size\":no_repeat_ngram_size,\n",
    "        \"num_beams\":num_beams,\n",
    "        \"return_full_text\":return_full_text,\n",
    "        \"n_gram_size\": 1\n",
    "    }\n",
    "    \n",
    "    s = time.time()\n",
    "    response = inference(prompt, params=params)\n",
    "    #print(response)\n",
    "    proc_time = time.time()-s\n",
    "    #print(f\"Processing time was {proc_time} seconds\")\n",
    "    return response\n",
    "\n",
    "\n",
    "j = 0\n",
    "for i in poll_ids:\n",
    "    j += 1\n",
    "    if j == 10:\n",
    "        break\n",
    "    if session.query(Question).filter_by(pollid=i).first() is None:\n",
    "        prompt = make_prompt_from_answers(i)\n",
    "        resp = infer(prompt)\n",
    "        question = Question(pollid=int(i), name=resp[0]['generated_text'][len(prompt):])\n",
    "        session.add(question)\n",
    "        session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9429ed0ccb4161b1b0d7fb517054d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d31e1bb49c4ae98ddefa81876052b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saadi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\saadi\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a266f532f34afaa158dc139f31761f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/24.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe1d92e9c084eecadd2462c29a10db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/798 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf5e59f9b9b46969c89f8e379b8c578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513ec3cfed304a40bf6b7ecd01145919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1a29e4e4e44a1ea30549e3368fa704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37765705b3904a1dac2c752ba7114a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80b21f3f0be410399dee3e6ec0a4e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/430 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\saadi\\\\.cache\\\\huggingface\\\\hub\\\\models--bigscience--mt0-base\\\\snapshots\\\\e91efc242bd5628ded2f46827a009379bb0ca811'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "snapshot_download(repo_id=\"bigscience/mt0-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "526a7a986be34c6aa5d2765e86ea11ad1e541f44b43fc99ae4544a937b9b911e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
